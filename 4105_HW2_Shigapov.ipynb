{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TreeLiquid/Intro-to-ML/blob/main/4105_HW2_Shigapov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "dp9mhzq_1FbP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Processing"
      ],
      "metadata": {
        "id": "er189TEZh2e2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOP0OGaVy9hc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Obtain file data\n",
        "filepath = '/content/drive/My Drive/Fall-2023/Intro-to-ML/Datasets/Housing.csv'\n",
        "HD = pd.DataFrame(pd.read_csv(filepath))\n",
        "HD.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HD.shape"
      ],
      "metadata": {
        "id": "yev9cH9XXuML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Null Checking\n",
        "HD.isnull().sum()*100/HD.shape[0]"
      ],
      "metadata": {
        "id": "Yie9xuWgZAQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting True/False to 1/0\n",
        "varlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "#Map Func (I am not converting furnishing status as the HW doesn't require me to)\n",
        "def binary_map(x):\n",
        "  return x.map({'yes': 1, 'no': 0})\n",
        "\n",
        "HD[varlist] = HD[varlist].apply(binary_map)\n",
        "HD.head()"
      ],
      "metadata": {
        "id": "WGjALIHhX-fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1A"
      ],
      "metadata": {
        "id": "RWjpgdV03xJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training & Test Set Setup + Feature scaling\n",
        "np.random.seed(0)\n",
        "HD_train, HD_test = train_test_split(HD, train_size = 0.8, test_size = 0.2, random_state = 100)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']\n",
        "HD_train[num_vars] = scaler.fit_transform(HD_train[num_vars])\n",
        "HD_train.head()\n"
      ],
      "metadata": {
        "id": "qCIvUbCUZ79M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Set\n",
        "y_train = HD_train.pop('price')\n",
        "x_train = HD_train\n",
        "print(x_train.head())\n",
        "\n",
        "#Validation Sets\n",
        "y_val = HD_test.pop('price')\n",
        "x_val = HD_test\n",
        "x_val.head()"
      ],
      "metadata": {
        "id": "pG8meDK8fSBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "gKzrkICV2pcd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#Functions (Adjusted function names to fit HW requirements better :))\n",
        "def compute_price(x, y, theta):\n",
        " \"\"\"\n",
        " Compute price for linear regression.\n",
        " Input Parameters\n",
        " ----------------\n",
        " X : 2D array where each row represent the training example and each column represent\n",
        " m= number of training examples\n",
        " n= number of features (including X_0 column of ones)\n",
        " y : 1D array of labels/target value for each traing example. dimension(1 x m)\n",
        " theta : 1D array of fitting parameters or weights. Dimension (1 x n)\n",
        "\n",
        " Output Parameters\n",
        " -----------------\n",
        " J : Scalar value.\n",
        " \"\"\"\n",
        " predictions = x.dot(theta)\n",
        " errors = np.subtract(predictions, y)\n",
        " sqrErrors = np.square(errors)\n",
        " J = 1 / (2 * m) * np.sum(sqrErrors)\n",
        " return J\n",
        "\n",
        "def gradient_descent(x, y, theta, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Compute price for linear regression.\n",
        "\n",
        "    Input Parameters\n",
        "    ----------------\n",
        "    X : 2D array where each row represent the training example and each column represent\n",
        "    m= number of training examples\n",
        "    n= number of features (including X_0 column of ones)\n",
        "    y : 1D array of labels/target value for each traing example. dimension(m x 1)\n",
        "    theta : 1D array of fitting parameters or weights. Dimension (1 x n)\n",
        "    alpha : Learning rate. Scalar value\n",
        "    iterations: No of iterations. Scalar value.\n",
        "\n",
        "    Output Parameters\n",
        "    -----------------\n",
        "    theta : Final Value. 1D array of fitting parameters or weights. Dimension (1 x n)\n",
        "    price_estimate: Conatins value of cost for each iteration. 1D array. Dimansion(m x 1)\n",
        "    \"\"\"\n",
        "    price_estimate = np.zeros(iterations)\n",
        "    for i in range(iterations):\n",
        "        predictions = x.dot(theta)\n",
        "        errors = np.subtract(predictions, y)\n",
        "        sum_delta = (alpha / m) * x.transpose().dot(errors);\n",
        "        theta = theta - sum_delta;\n",
        "        price_estimate[i] = compute_price(x, y, theta)\n",
        "    return theta, price_estimate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "YXXQZbY2syNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters\n",
        "m = len(y_train)\n",
        "theta = np.zeros((13,m), dtype=np.int8)\n",
        "print(theta)\n",
        "\n",
        "#Graph\n",
        "X_train1 = x_train['area'] + x_train['bedrooms'] + x_train['bathrooms'] + x_train['stories'] + x_train['parking']\n",
        "\n",
        "plt.scatter(X_train1, y_train, color='red',marker= '+')\n",
        "#plt.scatter(x_train['bedrooms'], y_train, color='blue',marker= '+')\n",
        "#plt.scatter(, y_train, color='green',marker= '+')\n",
        "#plt.scatter(, y_train, color='yellow',marker= '+')\n",
        "#plt.scatter(x_train['parking'], y_train, color='black',marker= '+')\n",
        "plt.grid()\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Scatter plot of training data')"
      ],
      "metadata": {
        "id": "SZOQvg6Ohn3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Descent\n",
        "itr = 150;\n",
        "a = 0.01;\n",
        "\n",
        "#theta, price_estimate = gradient_descent(X_train1, y_train, theta, a, itr)\n",
        "#print('Final value of Theta =', theta)\n",
        "#print('Price estimate  =', price_estimate)\n"
      ],
      "metadata": {
        "id": "1xLMZbAZuY8y"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and Validation Losses for 1A\n",
        "x_train_1A = np.c_[np.ones((len(x_train), 1)), x_train]\n",
        "x_val_1A = np.c_[np.ones((len(x_val), 1)), x_val]\n",
        "\n",
        "train_losses_1A = []\n",
        "val_losses_1A = []\n",
        "\n",
        "for iteration in range(itr):\n",
        "    gradients = 2/m * x_train_1A.T.dot(x_train_1A.dot(theta) - y_train)\n",
        "    theta -= a * gradients\n",
        "\n",
        "    train_loss = (1/m) * np.sum(np.square(x_train_1A.dot(theta) - y_train))\n",
        "    val_loss = (1/len(x_val)) * np.sum(np.square(x_val.dot(theta) - y_val))\n",
        "\n",
        "    train_losses_1A.append(train_loss)\n",
        "    val_losses_1A.append(val_loss)"
      ],
      "metadata": {
        "id": "ls9sBPqewQhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses_1A, label=\"Training Loss\")\n",
        "plt.plot(val_losses_1A, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss Over Iterations (For 1A)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q3XFudfjBlBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1b"
      ],
      "metadata": {
        "id": "ExEqip-CD0cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Organization\n",
        "HD_train_1b, HD_test_1b = train_test_split(HD, train_size = 0.8, test_size = 0.2, random_state = 100)\n",
        "\n",
        "#mainroad, guestroom, basement, hotwaterheating, airconditioning, prefarea\n",
        "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'price','mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']\n",
        "HD_train_1b[num_vars] = scaler.fit_transform(HD_train_1b[num_vars])\n",
        "HD_train_1b = HD_train_1b.drop(columns = ['furnishingstatus'])\n",
        "HD_test_1b = HD_test_1b.drop(columns = ['furnishingstatus'])\n",
        "HD_train_1b.head()"
      ],
      "metadata": {
        "id": "jMCeLfzRD3Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Set\n",
        "y_train_1b = HD_train_1b.pop('price')\n",
        "x_train_1b = HD_train_1b\n",
        "#x_train_1b.head()\n",
        "print(x_train_1b.shape)\n",
        "print(y_train_1b.shape)\n",
        "\n",
        "#Validation Sets\n",
        "y_val_1b = HD_test_1b.pop('price')\n",
        "x_val_1b = HD_test_1b\n",
        "print(x_val_1b.shape)\n",
        "print(y_val_1b.shape)\n",
        "x_val_1b.head()"
      ],
      "metadata": {
        "id": "MY0Rip9qFmzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and Validation Losses for 1B\n",
        "m = len(x_train_1b)\n",
        "itr_1B = 150\n",
        "a_1B = 0.01\n",
        "theta_1B = np.zeros((12,len(x_train_1b)))\n",
        "\n",
        "x_train_1B = np.c_[np.ones((len(x_train_1b), 1)), x_train_1b]\n",
        "x_val_1B = np.c_[np.ones((len(x_val_1b), 1)), x_val_1b]\n",
        "\n",
        "train_losses_1B = []\n",
        "val_losses_1B = []\n",
        "\n",
        "for iteration in range(itr_1B):\n",
        "    gradients_1B = 2/m * x_train_1B.T.dot(x_train_1B.dot(theta_1B) - y_train_1b)\n",
        "    theta -= a_1B * gradients_1B\n",
        "\n",
        "    train_loss_1b = (1/m) * np.sum(np.square(x_train_1b.dot(theta_1B) - y_train_1b))\n",
        "    val_loss_1b = (1/len(x_val_1B)) * np.sum(np.square(x_val_1B.dot(theta) - y_val_1b))\n",
        "\n",
        "    train_losses_1B.append(train_loss_1b)\n",
        "    val_losses_1B.append(val_loss_1b)"
      ],
      "metadata": {
        "id": "SQzaxrGNSVPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses_1B, label=\"Training Loss\")\n",
        "plt.plot(val_losses_1B, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss Over Iterations (For 1B)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1cSdlFRTVOU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2A"
      ],
      "metadata": {
        "id": "BEKtrd-RWN_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing: Normilaztion (0-1) and Standardization (mean removal)\n",
        "scaler = MinMaxScaler() #Normilization\n",
        "mScaler = preprocessing.StandardScaler() #Standardizer\n",
        "HD_train_2A_N, HD_test_2A_N = train_test_split(HD, train_size = 0.8, test_size = 0.2, random_state = 100)\n",
        "HD_train_2A_S, HD_test_2A_S = train_test_split(HD, train_size = 0.8, test_size = 0.2, random_state = 100)"
      ],
      "metadata": {
        "id": "iVRyjGaQZBWu"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2A - Normalization"
      ],
      "metadata": {
        "id": "sc1wtzhGfySI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Norming\n",
        "num_vars_2A_N = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']\n",
        "HD_train_2A_N[num_vars_2A_N] = scaler.fit_transform(HD_train_2A_N[num_vars_2A_N])\n",
        "HD_train_2A_N.head()"
      ],
      "metadata": {
        "id": "zYJDI5WWWTPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Delaring Sets & Removing \"extra\" Data\n",
        "#Training Set [Normalizing]\n",
        "y_train_2A_N = HD_train_2A_N.pop('price')\n",
        "x_train_2A_N = HD_train_2A_N\n",
        "print(x_train_2A_N.shape)\n",
        "print(y_train_2A_N.shape)\n",
        "\n",
        "\n",
        "\n",
        "x_train_2A_N.head()"
      ],
      "metadata": {
        "id": "WgrZplGkbuMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation Sets [Normalizing]\n",
        "y_val_2A_N = HD_test_2A_N.pop('price')\n",
        "x_val_2A_N = HD_test_2A_N\n",
        "print(x_val_2A_N.shape)\n",
        "print(y_val_2A_N.shape)\n",
        "\n",
        "\n",
        "\n",
        "x_train_2A_N.head()"
      ],
      "metadata": {
        "id": "VQs4jnFQcob3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta_2A_N =\n",
        "a_2A_N =\n",
        "itr_2A_N ="
      ],
      "metadata": {
        "id": "4VIILPwlhGke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and Validation Losses for 2A\n",
        "x_train_2A_N = np.c_[np.ones((len(x_train_2A_N), 1)), x_train_2A_N]\n",
        "x_val_2A_N = np.c_[np.ones((len(x_val_2A_N), 1)), x_val_2A_N]\n",
        "\n",
        "train_losses_2A_N = []\n",
        "val_losses_2A_N = []\n",
        "\n",
        "for iteration in range(itr_2A_N):\n",
        "    gradients_2A_N = 2/m * x_train_2A_N.T.dot(x_train_2A_N.dot(theta_2A_N) - y_train_2A_N)\n",
        "    theta_2A_N -= a_2A_N * gradients_2A_N\n",
        "\n",
        "    train_loss_2A_N = (1/m) * np.sum(np.square(x_train_2A_N.dot(theta_2A_N) - y_train_2A_N))\n",
        "    val_loss_2A_N = (1/len(x_val_2A_N)) * np.sum(np.square(x_val_2A_N.dot(theta_2A_N) - y_val_2A_N))\n",
        "\n",
        "    train_losses_2A_N.append(train_loss_2A_N)\n",
        "    val_losses_2A_N.append(val_loss_2A_N)"
      ],
      "metadata": {
        "id": "UX__695Hf8Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and validation losses for both training and validation set based on input standardization and input normalization\n",
        "plt.plot(train_losses_2A_N, label=\"Training Loss\")\n",
        "plt.plot(val_losses_2A_N, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss Over Iterations (For 2A)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rRHSSQP0iJz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2A - Standardization"
      ],
      "metadata": {
        "id": "f_RTaFLKf3P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardizing\n",
        "num_vars_2A_S = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']\n",
        "HD_train_2A_S[num_vars_2A_S] = mScaler.fit_transform(HD_train_2A_S[num_vars_2A_S])\n",
        "HD_train_2A_S.head()"
      ],
      "metadata": {
        "id": "Nf4lPGNca0iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Delaring Sets & Removing \"extra\" Data\n",
        "#Training Set [Standardizing]\n",
        "y_train_2A_S = HD_train_2A_S.pop('price')\n",
        "x_train_2A_S = HD_train_2A_S\n",
        "print(x_train_2A_S.shape)\n",
        "print(y_train_2A_S.shape)\n",
        "\n",
        "\n",
        "\n",
        "x_train_2A_S.head()"
      ],
      "metadata": {
        "id": "2iSsTe9VdHWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation Set [Standardizing]\n",
        "y_val_2A_S = HD_test_2A_S.pop('price')\n",
        "x_val_2A_S = HD_test_2A_S\n",
        "print(x_val_2A_S.shape)\n",
        "print(y_val_2A_S.shape)\n",
        "\n",
        "\n",
        "\n",
        "x_train_2A_S.head()"
      ],
      "metadata": {
        "id": "mH9vRgZydHdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta_2A_S = []\n",
        "a_2A_S = 100\n",
        "itr_2A_S = 0.1"
      ],
      "metadata": {
        "id": "g1hERcsKi9ZS"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and Validation Losses for 2A\n",
        "x_train_2A_S = np.c_[np.ones((len(x_train_2A_S), 1)), x_train_2A_S]\n",
        "x_val_2A_S = np.c_[np.ones((len(x_val_2A_S), 1)), x_val_2A_S]\n",
        "\n",
        "train_losses_2A_S = []\n",
        "val_losses_2A_S = []\n",
        "\n",
        "for iteration in range(itr_2A_S):\n",
        "    gradients_2A_S = 2/m * x_train_2A_S.T.dot(x_train_2A_S.dot(theta_2A_S) - y_train_2A_S)\n",
        "    theta_2A_S -= a_2A_S * gradients_2A_S\n",
        "\n",
        "    train_loss_2A_S = (1/m) * np.sum(np.square(x_train_2A_S.dot(theta_2A_S) - y_train_2A_S))\n",
        "    val_loss_2A_S = (1/len(x_val_2A_S)) * np.sum(np.square(x_val_2A_S.dot(theta_2A_S) - y_val_2A_S))\n",
        "\n",
        "    train_losses_2A_S.append(train_loss_2A_S)\n",
        "    val_losses_2A_S.append(val_loss_2A_S)"
      ],
      "metadata": {
        "id": "iXHe8id5i9jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and validation losses for both training and validation set based on input standardization and input normalization [Stando]\n",
        "plt.plot(train_losses_2A_S, label=\"Training Loss\")\n",
        "plt.plot(val_losses_2A_S, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss Over Iterations (For 2A)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "feLGambDi-aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2B"
      ],
      "metadata": {
        "id": "qButDwsukpNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HD_train_2B_N, HD_test_2B_N = train_test_split(HD, train_size = 0.8, test_size = 0.2, random_state = 100)\n",
        "HD_train_2B_S, HD_test_2B_S = train_test_split(HD, train_size = 0.8, test_size = 0.2, random_state = 100)"
      ],
      "metadata": {
        "id": "jVkDmgZzkvKQ"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2B - Normalization"
      ],
      "metadata": {
        "id": "awwBLegtkwJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Norming\n",
        "num_vars_2B_N = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']\n",
        "HD_train_2B_N[num_vars_2B_N] = scaler.fit_transform(HD_train_2B_N[num_vars_2B_N])\n",
        "HD_train_2B_N.head()"
      ],
      "metadata": {
        "id": "dkNxa6n3k-qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Delaring Sets & Removing \"extra\" Data\n",
        "#Training Set [Normalizing]\n",
        "y_train_2B_N = HD_train_2B_N.pop('price')\n",
        "x_train_2B_N = HD_train_2B_N\n",
        "print(x_train_2B_N.shape)\n",
        "print(y_train_2B_N.shape)\n",
        "\n",
        "\n",
        "\n",
        "x_train_2B_N.head()"
      ],
      "metadata": {
        "id": "vWG_nz7HnubK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation Sets [Normalizing]\n",
        "y_val_2B_N = HD_test_2B_N.pop('price')\n",
        "x_val_2B_N = HD_test_2B_N\n",
        "print(x_val_2B_N.shape)\n",
        "print(y_val_2B_N.shape)\n",
        "\n",
        "\n",
        "\n",
        "x_train_2B_N.head()"
      ],
      "metadata": {
        "id": "vETFlmBOn4FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta_2B_N =\n",
        "a_2B_N =\n",
        "itr_2B_N ="
      ],
      "metadata": {
        "id": "qALemqVDoFur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and Validation Losses for 2B [Norm]\n",
        "x_train_2B_N = np.c_[np.ones((len(x_train_2B_N), 1)), x_train_2B_N]\n",
        "x_val_2B_N = np.c_[np.ones((len(x_val_2B_N), 1)), x_val_2B_N]\n",
        "\n",
        "train_losses_2B_N = []\n",
        "val_losses_2B_N = []\n",
        "\n",
        "for iteration in range(itr_2B_N):\n",
        "    gradients_2B_N = 2/m * x_train_2B_N.T.dot(x_train_2B_N.dot(theta_2B_N) - y_train_2B_N)\n",
        "    theta_2B_N -= a_2B_N * gradients_2B_N\n",
        "\n",
        "    train_loss_2B_N = (1/m) * np.sum(np.square(x_train_2B_N.dot(theta_2B_N) - y_train_2B_N))\n",
        "    val_loss_2B_N = (1/len(x_val_2B_N)) * np.sum(np.square(x_val_2B_N.dot(theta_2B_N) - y_val_2B_N))\n",
        "\n",
        "    train_losses_2B_N.append(train_loss_2B_N)\n",
        "    val_losses_2B_N.append(val_loss_2B_N)"
      ],
      "metadata": {
        "id": "M7Q1zeEioGcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses_2B_N, label=\"Training Loss\")\n",
        "plt.plot(val_losses_2B_N, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss Over Iterations (For 1B)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tkywIvH4ooXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2B - Standardization"
      ],
      "metadata": {
        "id": "Yv1siugXosv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stando\n",
        "num_vars_2B_S = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']\n",
        "HD_train_2B_S[num_vars_2B_N] = scaler.fit_transform(HD_train_2B_S[num_vars_2B_S])\n",
        "HD_train_2B_S.head()"
      ],
      "metadata": {
        "id": "5nH6OpHOoxfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Delaring Sets & Removing \"extra\" Data\n",
        "#Training Set [Stando]\n",
        "y_train_2B_S = HD_train_2B_S.pop('price')\n",
        "x_train_2B_S = HD_train_2B_S\n",
        "print(x_train_2B_S.shape)\n",
        "print(y_train_2B_S.shape)\n",
        "\n",
        "\n",
        "\n",
        "x_train_2B_S.head()"
      ],
      "metadata": {
        "id": "rlh-Dh9wo-Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation Sets [Stando]\n",
        "y_val_2B_S = HD_test_2B_S.pop('price')\n",
        "x_val_2B_S = HD_test_2B_S\n",
        "print(x_val_2B_S.shape)\n",
        "print(y_val_2B_S.shape)\n",
        "\n",
        "\n",
        "\n",
        "x_train_2B_S.head()"
      ],
      "metadata": {
        "id": "h-IiPO68pii0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta_2B_S =\n",
        "a_2B_S =\n",
        "itr_2B_S ="
      ],
      "metadata": {
        "id": "ZBJ3T0xRpvXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and Validation Losses for 2B [Stando]\n",
        "x_train_2B_S = np.c_[np.ones((len(x_train_2B_S), 1)), x_train_2B_S]\n",
        "x_val_2B_S = np.c_[np.ones((len(x_val_2B_S), 1)), x_val_2B_S]\n",
        "\n",
        "train_losses_2B_S = []\n",
        "val_losses_2B_S = []\n",
        "\n",
        "for iteration in range(itr_2B_S):\n",
        "    gradients_2B_S = 2/m * x_train_2B_S.T.dot(x_train_2S_N.dot(theta_2B_S) - y_train_2B_S)\n",
        "    theta_2B_S -= a_2B_S * gradients_2B_S\n",
        "\n",
        "    train_loss_2B_S = (1/m) * np.sum(np.square(x_train_2B_S.dot(theta_2B_S) - y_train_2B_S))\n",
        "    val_loss_2B_S = (1/len(x_val_2B_S)) * np.sum(np.square(x_val_2B_S.dot(theta_2B_S) - y_val_2B_S))\n",
        "\n",
        "    train_losses_2B_S.append(train_loss_2B_S)\n",
        "    val_losses_2B_S.append(val_loss_2B_S)"
      ],
      "metadata": {
        "id": "0Sl2YgvQp1_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses_2B_S, label=\"Training Loss\")\n",
        "plt.plot(val_losses_2B_S, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss Over Iterations (For 1B)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PGzosK_-qgh1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOICWu4alNmiC24w0OnNpR3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}